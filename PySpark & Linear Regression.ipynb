{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a SPARK session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform.startswith('win'):\n",
    "    os.chdir(r\"C:\\Users\\Thatoi\\SparkPythonDoBigDataAnalytics-Resources\\SparkPythonDoBigDataAnalytics-Resources\")\n",
    "    os.environ['SPARK_HOME'] = 'C:/Users/Thatoi/Downloads/spark-3.0.0-preview2-bin-hadoop2.7'\n",
    "\n",
    "# create a variable for root path\n",
    "SPARK_HOME = os.environ['SPARK_HOME'] \n",
    "\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"pyspark.zip\"))\n",
    "#sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"py4j-0.10.1-src.zip\")) 'Doesn't work : Try conda install py4j'\n",
    "\n",
    "#Initialize SparkSession and SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "#Create a Spark Session\n",
    "SpSession = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"V2 Maestros\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.cores.max\",\"2\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///c:/Users/temp/spark-warehouse\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Get the Spark Context from Spark Session    \n",
    "SpContext = SpSession.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MPG,CYLINDERS,DISPLACEMENT,HORSEPOWER,WEIGHT,ACCELERATION,MODELYEAR,NAME',\n",
       " '18,8,307,130,3504,12,70,chevrolet chevelle malibu',\n",
       " '15,8,350,165,3693,11.5,70,buick skylark 320',\n",
       " '18,8,318,150,3436,11,70,plymouth satellite',\n",
       " '16,8,304,150,3433,12,70,amc rebel sst']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto = SpContext.textFile('auto-miles-per-gallon.csv')\n",
    "auto.cache()\n",
    "auto.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove header line\n",
    "datalines = auto.filter(lambda line : 'CYLINDERS' not in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleanup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "#creating a default value for HP\n",
    "avgHP = SpContext.broadcast(80.0)\n",
    "avgMPG = SpContext.broadcast(140.0)\n",
    "\n",
    "#Function to clean up data\n",
    "def cleanup (inputstr):\n",
    "    global avgHP\n",
    "    global avgMPG\n",
    "    \n",
    "    attlist = inputstr.split(\",\")\n",
    "    \n",
    "    #Replace ? with the average\n",
    "    hpvalue = attlist[3]\n",
    "    mpgvalue = attlist[0]\n",
    "    \n",
    "    if hpvalue == '?':\n",
    "        hpvalue = avgHP.value\n",
    "    if mpgvalue == '?':\n",
    "        mpgvalue = avgMPG.value\n",
    "        \n",
    "        \n",
    "    #Replace row with converted data\n",
    "    values = Row( MPG = float(mpgvalue),\\\n",
    "                  CYLINDERS = float(attlist[1]),\\\n",
    "                  DISPLACEMENT = float(attlist[2]),\\\n",
    "                  HORSEPOWER = float(hpvalue),\\\n",
    "                  WEIGHT = float(attlist[4]),\\\n",
    "                  ACCELERATION = float(attlist[5]),\\\n",
    "                  MODELYEAR = float(attlist[6]),\\\n",
    "                  NAME = attlist[7])\n",
    "    return values        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the map\n",
    "cleandata = datalines.map(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ACCELERATION=12.0, CYLINDERS=8.0, DISPLACEMENT=307.0, HORSEPOWER=130.0, MODELYEAR=70.0, MPG=18.0, NAME='chevrolet chevelle malibu', WEIGHT=3504.0),\n",
       " Row(ACCELERATION=11.5, CYLINDERS=8.0, DISPLACEMENT=350.0, HORSEPOWER=165.0, MODELYEAR=70.0, MPG=15.0, NAME='buick skylark 320', WEIGHT=3693.0),\n",
       " Row(ACCELERATION=11.0, CYLINDERS=8.0, DISPLACEMENT=318.0, HORSEPOWER=150.0, MODELYEAR=70.0, MPG=18.0, NAME='plymouth satellite', WEIGHT=3436.0),\n",
       " Row(ACCELERATION=12.0, CYLINDERS=8.0, DISPLACEMENT=304.0, HORSEPOWER=150.0, MODELYEAR=70.0, MPG=16.0, NAME='amc rebel sst', WEIGHT=3433.0),\n",
       " Row(ACCELERATION=10.5, CYLINDERS=8.0, DISPLACEMENT=302.0, HORSEPOWER=140.0, MODELYEAR=70.0, MPG=17.0, NAME='ford torino', WEIGHT=3449.0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleandata.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descriptive analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf=SpSession.createDataFrame(cleandata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|              MPG|         CYLINDERS|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|              398|               398|\n",
      "|   mean|23.51457286432161| 5.454773869346734|\n",
      "| stddev|7.815984312565782|1.7010042445332125|\n",
      "|    min|              9.0|               3.0|\n",
      "|    max|             46.6|               8.0|\n",
      "+-------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleandf.select('MPG','CYLINDERS').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to MPG for  ACCELERATION 0.4202889121016501\n",
      "Correlation to MPG for  CYLINDERS -0.7753962854205548\n",
      "Correlation to MPG for  DISPLACEMENT -0.8042028248058979\n",
      "Correlation to MPG for  HORSEPOWER -0.7746308409203807\n",
      "Correlation to MPG for  MODELYEAR 0.5792671330833091\n",
      "Correlation to MPG for  MPG 1.0\n",
      "Correlation to MPG for  WEIGHT -0.8317409332443347\n"
     ]
    }
   ],
   "source": [
    "#Find correlation between predictors and target\n",
    "for i in cleandf.columns:\n",
    "    if not( isinstance(cleandf.select(i).take(1)[0][0], str)) :\n",
    "        print( \"Correlation to MPG for \", i, cleandf.stat.corr('MPG',i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparin data for ML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformtolabelpoint(row):\n",
    "    lp = (row['MPG'],Vectors.dense([row['ACCELERATION'],row['DISPLACEMENT'],row['WEIGHT']]))\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "autolp = cleandata.map(transformtolabelpoint)\n",
    "autodf = SpSession.createDataFrame(autolp,['label','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|label|           features|\n",
      "+-----+-------------------+\n",
      "| 18.0|[12.0,307.0,3504.0]|\n",
      "| 15.0|[11.5,350.0,3693.0]|\n",
      "| 18.0|[11.0,318.0,3436.0]|\n",
      "| 16.0|[12.0,304.0,3433.0]|\n",
      "| 17.0|[10.5,302.0,3449.0]|\n",
      "| 15.0|[10.0,429.0,4341.0]|\n",
      "| 14.0| [9.0,454.0,4354.0]|\n",
      "| 14.0| [8.5,440.0,4312.0]|\n",
      "| 14.0|[10.0,455.0,4425.0]|\n",
      "| 15.0| [8.5,390.0,3850.0]|\n",
      "+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "autodf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training & Test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training,test) = autodf.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 104\n"
     ]
    }
   ],
   "source": [
    "print(training.count(), test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient[0.15740204680068842,-0.009686820784601998,-0.0063927133208949915]\n",
      "Intercept42.03496486348603\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(maxIter =10)\n",
    "lrModel = lr.fit(training)\n",
    "print('Coefficient'+ str(lrModel.coefficients))\n",
    "print('Intercept'+ str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+-------------------+\n",
      "|        prediction|label|           features|\n",
      "+------------------+-----+-------------------+\n",
      "|11.248966060308561| 10.0|[14.0,360.0,4615.0]|\n",
      "| 11.85128594317737| 12.0|[12.5,350.0,4499.0]|\n",
      "|11.511988846778252| 13.0|[12.0,400.0,4464.0]|\n",
      "| 7.190514641853234| 13.0|[12.0,400.0,5140.0]|\n",
      "|11.989509850015374| 13.0|[13.5,350.0,4502.0]|\n",
      "| 14.17766673542415| 13.0|[16.0,302.0,4294.0]|\n",
      "|11.219892849306127| 14.0| [9.0,454.0,4354.0]|\n",
      "| 17.42625156884853| 14.0|[11.5,304.0,3672.0]|\n",
      "|14.285604074524258| 14.0|[13.0,351.0,4129.0]|\n",
      "|12.587160214562914| 14.0|[13.5,318.0,4457.0]|\n",
      "| 14.20448726490223| 14.0|[13.5,351.0,4154.0]|\n",
      "|10.988952464492048| 14.0|[13.5,351.0,4657.0]|\n",
      "|15.095092299903357| 14.0|[14.0,318.0,4077.0]|\n",
      "|15.552527422088655| 14.0|[14.5,302.0,4042.0]|\n",
      "| 16.77680322597082| 15.0|[12.5,318.0,3777.0]|\n",
      "| 22.49441091256257| 15.0|[19.5,250.0,3158.0]|\n",
      "|20.978910532838377| 15.0|[21.0,250.0,3432.0]|\n",
      "|12.307528407462996| 16.0| [9.5,400.0,4278.0]|\n",
      "|10.129174305915328| 16.0|[11.5,400.0,4668.0]|\n",
      "|14.840947779919706| 16.0|[14.0,302.0,4141.0]|\n",
      "+------------------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction= lrModel.transform(test)\n",
    "prediction.select('prediction','label','features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6756183014210286"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol = 'prediction', labelCol ='label', metricName = 'r2')\n",
    "evaluator.evaluate(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
